{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing necesaary libraries and packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tqdm.notebook import tqdm\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-04T02:42:06.680008Z","iopub.execute_input":"2021-07-04T02:42:06.680783Z","iopub.status.idle":"2021-07-04T02:42:15.787060Z","shell.execute_reply.started":"2021-07-04T02:42:06.680659Z","shell.execute_reply":"2021-07-04T02:42:15.786112Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Importing necessary datasets and arrange them into Train, test,and Validation usecases","metadata":{}},{"cell_type":"code","source":"train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n# train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n# train2['toxic'] = train2.toxic.round().astype(int)\n\nvalid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\ntest = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n# sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:46:18.070514Z","iopub.execute_input":"2021-07-04T02:46:18.070964Z","iopub.status.idle":"2021-07-04T02:46:22.108747Z","shell.execute_reply.started":"2021-07-04T02:46:18.070923Z","shell.execute_reply":"2021-07-04T02:46:22.107519Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def def2(texts, tokenizer,maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts.tolist(), \n        return_token_type_ids=False,\n        pad_to_max_length=True,        \n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:46:22.110351Z","iopub.execute_input":"2021-07-04T02:46:22.110682Z","iopub.status.idle":"2021-07-04T02:46:22.116221Z","shell.execute_reply.started":"2021-07-04T02:46:22.110649Z","shell.execute_reply":"2021-07-04T02:46:22.115066Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Buiding a sequential model using Accuracy as metrics and Binary_Crossentropy as Loss function. ","metadata":{}},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:46:26.289924Z","iopub.execute_input":"2021-07-04T02:46:26.290421Z","iopub.status.idle":"2021-07-04T02:46:26.299142Z","shell.execute_reply.started":"2021-07-04T02:46:26.290374Z","shell.execute_reply":"2021-07-04T02:46:26.297653Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Start the TPU Accelerator!","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \",strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:46:30.609894Z","iopub.execute_input":"2021-07-04T02:46:30.610331Z","iopub.status.idle":"2021-07-04T02:46:36.019820Z","shell.execute_reply.started":"2021-07-04T02:46:30.610290Z","shell.execute_reply":"2021-07-04T02:46:36.018930Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:47:10.530990Z","iopub.execute_input":"2021-07-04T02:47:10.531471Z","iopub.status.idle":"2021-07-04T02:47:10.535983Z","shell.execute_reply.started":"2021-07-04T02:47:10.531433Z","shell.execute_reply":"2021-07-04T02:47:10.534942Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nbatch_size = 16 * strategy.num_replicas_in_sync\nmax_len = 192\nMODEL='../input/jplu-tf-xlm-roberta-large'\n# MODEL1 = 'xlmr.base'","metadata":{"execution":{"iopub.status.busy":"2021-07-04T03:16:52.530459Z","iopub.execute_input":"2021-07-04T03:16:52.531079Z","iopub.status.idle":"2021-07-04T03:16:52.537932Z","shell.execute_reply.started":"2021-07-04T03:16:52.531040Z","shell.execute_reply":"2021-07-04T03:16:52.536343Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T03:16:54.169947Z","iopub.execute_input":"2021-07-04T03:16:54.170405Z","iopub.status.idle":"2021-07-04T03:16:58.175159Z","shell.execute_reply.started":"2021-07-04T03:16:54.170357Z","shell.execute_reply":"2021-07-04T03:16:58.173920Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([\n    train1[['comment_text', 'toxic']],\n#     train2[['comment_text', 'toxic']].query('toxic==1'),\n#     train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000)\n])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T03:04:10.299042Z","iopub.execute_input":"2021-07-04T03:04:10.299534Z","iopub.status.idle":"2021-07-04T03:04:10.383545Z","shell.execute_reply.started":"2021-07-04T03:04:10.299493Z","shell.execute_reply":"2021-07-04T03:04:10.382293Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"x_train = def2(train.comment_text.values, tokenizer,maxlen=max_len)\nx_valid = def2(valid.comment_text.values, tokenizer,maxlen=max_len)\nx_test = def2(test.content.values, tokenizer,maxlen=max_len)\n\ny_train = train['toxic'].values\ny_valid = valid['toxic'].values","metadata":{"execution":{"iopub.status.busy":"2021-07-04T03:04:11.210441Z","iopub.execute_input":"2021-07-04T03:04:11.210855Z","iopub.status.idle":"2021-07-04T03:05:33.434442Z","shell.execute_reply.started":"2021-07-04T03:04:11.210822Z","shell.execute_reply":"2021-07-04T03:05:33.433087Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"xlmr_model = TFAutoModel.from_pretrained(MODEL)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T03:17:16.221041Z","iopub.execute_input":"2021-07-04T03:17:16.221605Z","iopub.status.idle":"2021-07-04T03:17:53.847058Z","shell.execute_reply.started":"2021-07-04T03:17:16.221559Z","shell.execute_reply":"2021-07-04T03:17:53.845740Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at ../input/jplu-tf-xlm-roberta-large were not used when initializing TFXLMRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFXLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at ../input/jplu-tf-xlm-roberta-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nvalidation_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T03:18:16.196975Z","iopub.execute_input":"2021-07-04T03:18:16.197463Z","iopub.status.idle":"2021-07-04T03:18:17.707009Z","shell.execute_reply.started":"2021-07-04T03:18:16.197426Z","shell.execute_reply":"2021-07-04T03:18:17.705711Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=192)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T03:18:22.820630Z","iopub.execute_input":"2021-07-04T03:18:22.821097Z","iopub.status.idle":"2021-07-04T03:19:49.802114Z","shell.execute_reply.started":"2021-07-04T03:18:22.821052Z","shell.execute_reply":"2021-07-04T03:19:49.800977Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at ../input/jplu-tf-xlm-roberta-large were not used when initializing TFXLMRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFXLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at ../input/jplu-tf-xlm-roberta-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T03:19:49.804104Z","iopub.execute_input":"2021-07-04T03:19:49.804491Z","iopub.status.idle":"2021-07-04T03:19:49.849748Z","shell.execute_reply.started":"2021-07-04T03:19:49.804454Z","shell.execute_reply":"2021-07-04T03:19:49.848327Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 192)]             0         \n_________________________________________________________________\ntfxlm_roberta_model_1 (TFXLM TFBaseModelOutputWithPool 559890432 \n_________________________________________________________________\ntf.__operators__.getitem (Sl (None, 1024)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1025      \n=================================================================\nTotal params: 559,891,457\nTrainable params: 559,891,457\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training my XLM-Roberta_Model with english only data.","metadata":{}},{"cell_type":"markdown","source":"Note: It took me more than 15 minutes for single epoch training.","metadata":{}},{"cell_type":"code","source":"steps = x_train.shape[0] // batch_size\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=steps,\n    validation_data=validation_dataset,\n    epochs=1,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T03:34:33.415179Z","iopub.execute_input":"2021-07-04T03:34:33.415694Z","iopub.status.idle":"2021-07-04T03:34:33.455022Z","shell.execute_reply.started":"2021-07-04T03:34:33.415650Z","shell.execute_reply":"2021-07-04T03:34:33.453266Z"},"trusted":true},"execution_count":35,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-76495043c207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n","\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'test_data'"],"ename":"TypeError","evalue":"fit() got an unexpected keyword argument 'test_data'","output_type":"error"}]},{"cell_type":"code","source":"output = model.predict(test_dataset, verbose=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training my model with multiple_language data!","metadata":{}},{"cell_type":"code","source":"steps = x_valid.shape[0] // batch_size\ntrain_history_2 = model.fit(\n    validation_dataset.repeat(),\n    steps_per_epoch=steps,\n    epochs=10\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = model.predict(test_dataset, verbose=1)\n# sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T03:35:11.256732Z","iopub.execute_input":"2021-07-04T03:35:11.257254Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"172/499 [=========>....................] - ETA: 47s","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}